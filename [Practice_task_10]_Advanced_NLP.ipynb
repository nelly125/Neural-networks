{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice task 10, Advanced NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "latex_envs": {
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 0
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSBfdcEo02EH"
      },
      "source": [
        "## Семинар 10: \"Современные модели для NLP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt2LcA_C02EJ"
      },
      "source": [
        "ФИО: Перфильева Нелли Андреевна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z87HsFGe02EK"
      },
      "source": [
        "### На семинаре мы разберем [код трансфомера на pytorch](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0m8IOq802E8"
      },
      "source": [
        "###  ДЗ [3 балла]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHSBBqYwfETJ"
      },
      "source": [
        "Обратите внимание, что в этой работе вам потребуется скачать модель весом ~150MB, также ее вычисление занимает определенное время, так что рекомендуется считать эту задачу на [google colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a7Twd_m09PH",
        "outputId": "0944bff3-f5e0-45ae-b7bb-bcc4d4daeffa"
      },
      "source": [
        "import torch\n",
        "!pip install --upgrade transformers\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "from transformers import *\n",
        "from torch.distributions.categorical import Categorical\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mEU6bzh02E9",
        "outputId": "a748bf8e-a634-4151-eda2-e957dcf25697"
      },
      "source": [
        "MODEL = (MobileBertForMaskedLM, MobileBertTokenizer, 'google/mobilebert-uncased')\n",
        "\n",
        "model_class, tokenizer_class, pretrained_weights = MODEL\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing MobileBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MobileBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjX-8e2X1RID",
        "outputId": "b494ea68-9d69-4283-bcb8-5f9d948acabc"
      },
      "source": [
        "input_ids = tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "print(input_ids)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V72DIYwd1yZS",
        "outputId": "88d37c41-c83a-4535-ff94-76ab658e27fd"
      },
      "source": [
        "tokenizer.decode(input_ids)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] here is some text to encode [SEP]'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rXSL-TZG6BF-",
        "outputId": "0e6a8f25-e0b0-4414-c382-c1f77bdf783f"
      },
      "source": [
        "input_ids[4] = tokenizer.mask_token_id\n",
        "tokenizer.decode(input_ids)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] here is some [MASK] to encode [SEP]'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1y3f8rh10bz"
      },
      "source": [
        "input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
        "with torch.no_grad():\n",
        "    res = model(input_batch)[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVwXZBe72Dws"
      },
      "source": [
        "prob = torch.nn.functional.softmax(res, dim=-1)\n",
        "new_ids = prob.max(-1)[1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6ilhBQmo5r0B",
        "outputId": "fa72ce29-f543-4385-8da7-cbfc630ddaec"
      },
      "source": [
        "tokenizer.decode(new_ids.numpy()[0, :].tolist())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'. here is some way to encode the'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvCPgNEg6XCl"
      },
      "source": [
        "GPT_TEXTS = [\n",
        "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
        "    \"A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\"\n",
        "    ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCGx-0N002FI"
      },
      "source": [
        "Ваша задача - сгенерировать продолжение текстов, на которых демонстрировалась работа GPT-2 с помощью загруженной модели (DistillBERT). Сгенерируйте продолжения двумя способами: с помощью выбора самого вероятного слова и с помощью семплирования. Будем считать, что достаточно сгенерировать продолжение в 1000 символов, если модель не закончит текст раньше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n32KVO1RjCr6"
      },
      "source": [
        "encoded_text = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
        "encoded_text.pop()\n",
        "new_token = 0\n",
        "\n",
        "while (new_token != 102 and len(encoded_text) < 100):\n",
        "    encoded_text.append(tokenizer.mask_token_id)\n",
        "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      pred = model(input)[0]\n",
        "    prob = torch.nn.functional.softmax(pred, dim = -1)\n",
        "    new_ids = prob.max(-1)[1][0]\n",
        "    new_token = new_ids[-1].item()\n",
        "    encoded_text[-1] = new_ids[-1].item()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ChYHsLMQ4nTd",
        "outputId": "e99d6a73-f186-4ddc-fd84-e0eadce62205"
      },
      "source": [
        "tokenizer.decode(encoded_text)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. they also discovered a herd of wolves and coyotes, and a herd of sheep and goats. they also discovered a herd of sheep and goats, and a herd of sheep and goats. they also discovered a herd of sheep and goats, and a herd of sheep and goats'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoEFcG8I5MiV"
      },
      "source": [
        "encoded_text = tokenizer.encode(GPT_TEXTS[1], add_special_tokens=True)\n",
        "encoded_text.pop()\n",
        "new_token = 0\n",
        "\n",
        "while (new_token != 102 and len(encoded_text) < 100):\n",
        "    encoded_text.append(tokenizer.mask_token_id)\n",
        "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      pred = model(input)[0]\n",
        "    prob = torch.nn.functional.softmax(pred, dim = -1)\n",
        "    new_ids = prob.max(-1)[1][0]\n",
        "    new_token = new_ids[-1].item()\n",
        "    encoded_text[-1] = new_ids[-1].item()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fEPqMRZr5OOH",
        "outputId": "d914f5fe-099c-4680-e936-199999547e55"
      },
      "source": [
        "tokenizer.decode(encoded_text)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] a train carriage containing controlled nuclear materials was stolen in cincinnati today. its whereabouts are unknown. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today. the train carriage was stolen in cincinnati today.'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH4h9gvfLweE"
      },
      "source": [
        "Семплирование"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnGkN8Wkapk7"
      },
      "source": [
        "encoded_text = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
        "encoded_text.pop()\n",
        "\n",
        "new_token = 0\n",
        "while (new_token != 102 and len(encoded_text) < 500):\n",
        "    encoded_text.append(tokenizer.mask_token_id)\n",
        "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      pred = model(input)[0]\n",
        "    probs = torch.nn.functional.softmax(pred, dim = -1)\n",
        "    new_ids = Categorical(probs).sample()[0] \n",
        "    new_token = new_ids[-1].item()\n",
        "    encoded_text[-1] = new_ids[-1].item()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb7LrkUjoyOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "fc9d4029-7061-4acc-b24c-9a08e6401160"
      },
      "source": [
        "tokenizer.decode(encoded_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english.... but all this was foolish to realize, and i think so highly or never would occur, and it was that impossible for something moving to be recent, particularly remote and remote brass completely, imagine me, who heard except words and varied sounds etc. and would not have known them before which little described by me literally. an eastern magnetic anomaly along the andes of the autumn 26th 9th 1910 - the publications had superseded the air, changing it practically instantly, which had invaded the antarctic ice and melted the winter vapor incoming from the waters in antarctica. magnetic anomaly dis into high explosive forces, which committed themselves to a new quantum of change. the break hinduism a solid point formed in which couple rude horns created a double - stripes significant and red - gold accessible. from soil, were made ceramic mercury - iron - magnesium. copper - silver - gold - silver - copper - gold - gold - silver - rich, tin - gold - tin - rich. melted ramen \", \" salt \", \" excellent \" water measures generated by a sand section. objects and internal systems and external systems and roadsides and construction sites. site mixtures, information and places of place. objects and other structures. rock. forms and microlithic structures. assembled base evil material ; material and natural ; natural ; natural. material ; natural and natural : formed ; determined ; and seeded. earth and heavens. earth and sky plants, trees, stars, cosmos and chromosomes. instruments and instruments and instruments e ogvindahol on a finger, assembled by a drawing. fluid, illustrations and other sketches. seeds, seeds and... and golden snow white with fire. the seed and its soil. life and death. sand and gravel, planks. roads, pathways and routes. road and pathway of egypt. geography and hydrology blve. geography and hydrology vol. 132. no. 44, august 1917 and onwards. workflow and production. the engineers and the builders … the engineers and builders. in chapters and parts and sections he attacked the entire world and his scholars and experts. well - edited but incompletely agreed. the principles in eastern swifts and arrows. the instruments, and contracts of the astronomer whirin\\'s school for a year a'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5QDXe9Dapsd"
      },
      "source": [
        "encoded_text = tokenizer.encode(GPT_TEXTS[1], add_special_tokens=True)\n",
        "encoded_text.pop()\n",
        "\n",
        "new_token = 0\n",
        "while (new_token != 102 and len(encoded_text) < 500):\n",
        "    encoded_text.append(tokenizer.mask_token_id)\n",
        "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      pred = model(input)[0]\n",
        "    probs = torch.nn.functional.softmax(pred, dim = -1)\n",
        "    new_ids = Categorical(probs).sample()[0] \n",
        "    new_token = new_ids[-1].item()\n",
        "    encoded_text[-1] = new_ids[-1].item()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSg1KEXlap0S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "7cb2e7ff-96bf-4a75-abd8-317189f9380d"
      },
      "source": [
        "tokenizer.decode(encoded_text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[CLS] a train carriage containing controlled nuclear materials was stolen in cincinnati today. its whereabouts are unknown. analytical explosions are evident there and machinery is missing. it is suspected to have been - - tofu liners, aircraft, groups arrested weapons, some ordnances contained in the vehicle. i. n. g. n. g. f... a. sp. f. a. w. s. n. m. t. y. - - bullet team... o. e. f. a. public enemy fabric past forms of warfare - - depression the pillow - white cap pack & those too, - - pretty sarah go, go go go, - - great steel - iron - and - steel - under, - - jodim ars music and bronzeware, - - earlier hungarian imported, - - old style - style - avatar / - - virgin polish, - - old america - style - platform'- 200 000, – pitches 514. 43th w = 1 p 1903 | | } - - then healyo'h2 / - - - finally, - - - and - - * now my dear lord hungry hell be i - - - - and correctly - - - cold icy, - – - * - * — and beyond the good, for none of our enemies have understood it, his late ruler had never feared it — and true for lucifer was not smart and violent ; indeed man, an eternal, was not saintly so! who it was then uponstoing or make my keep / / - - - - - - /... - i will give your pardon, and fire your weapons! bleed! you use wands or stones. not at all respecting yourself, why would you move you up overhame? cuts having been … celeste jean newcomer her place in the jordan expedition. - - welcome and judge the best such you need to receive, thanks for giving one to your daughters and grandchildren and emotional leaders, news updates, and parenting tips. - - - enjoy your food, food gets you better, and best is food the best is these days. - - apologies for our travels and fondly to herdenator agenas milestones legacies and plants and animals ancient and modern history value adds, abandon, and destruction spirituality and influence peoples love and naturalism oceans millennium, millennium, coast, ocean, coast, sea, and cape of the scale - - - / - - finding a new path to change\""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyfWoU8JPfVr"
      },
      "source": [
        "Другая модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IgTwcUzMlrI"
      },
      "source": [
        "MODEL = (DistilBertForMaskedLM, DistilBertTokenizer, 'distilbert-base-cased')\n",
        "\n",
        "model_class, tokenizer_class, pretrained_weights = MODEL\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSCO7ehyMq3z"
      },
      "source": [
        "encoded_text = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
        "encoded_text.pop()\n",
        "\n",
        "new_token = 0\n",
        "while (new_token != 102 and len(encoded_text) < 250):\n",
        "    encoded_text.append(tokenizer.mask_token_id)\n",
        "    input = torch.tensor(encoded_text).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      pred = model(input)[0]\n",
        "    probs = torch.nn.functional.softmax(pred, dim = -1)\n",
        "    new_ids = Categorical(probs).sample()[0] \n",
        "    new_token = new_ids[-1].item()\n",
        "    encoded_text[-1] = new_ids[-1].item()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "E7dZaBt9Mq6K",
        "outputId": "a9b56cee-0c24-4e2b-d417-109dc9a6b7f1"
      },
      "source": [
        "tokenizer.decode(encoded_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.ophone the languages communicating were closely language communication languages vs the ngolo dialect Standard spoke Dictionary languages vs vocabulary Universal → audit meanings suffered back to lands I → → soldiers & scouts → upwards order named Regiment → 1899 Town squad marine & 1st platoon Lt troop cop remember 4th platoon 7th platoon provisional during 7th platoon communist Commando lad platoon member seal vital farewell welcome soldier reminder outnumbered Dec 6th platoon evacuation reformed platoon SS 9th platoon drawn battalion lad platoon corps VIIIr 3rd platoon platoon soldier platoon signal platoon dunes accurate foe gets aid battlefield map 1895 vascular Extended map soldier platoon Security platoon scout normal platoon mates carried bones alive recovered crew seal alive devastated warrior 1931 Mission backup head kills ring graveyard fate emergency matters sealed destiny passage was relieved defeats helm job commander commander subordinate guidance staff commander commander executive assuming helicopter command commander helicopter platoon commander platoon commander functions Commandant petty commander troops commander engineer platoon commander chef commander base commander commander super admiral commander commander officer headquarters airborne headquarters marshal and dispatch sergeant backup powers cerebral commander commander ship planet devastated King plane Chief author martial accumulated Cannon Doctor commanders'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNHu0Uhf02FV"
      },
      "source": [
        "#### Feedback (опционально)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBZdRJeB02FW"
      },
      "source": [
        "Здесь вы можете оставить список опечаток из лекции или семинара:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNp4g0rW02FX"
      },
      "source": [
        "Здесь вы можете оставить комментарии по лекции или семинару:"
      ]
    }
  ]
}